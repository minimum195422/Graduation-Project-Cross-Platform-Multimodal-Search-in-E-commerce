from pathlib import Path
import os
import json
import re
import time
import random
import urllib.parse
import requests
import chromedriver_autoinstaller

from datetime import datetime
from dotenv import load_dotenv

from seleniumwire import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException


from proxy import RotatingProxy
from human_simulator import HumanBehaviorSimulator
from rabbitmq_connector import RabbitMQConnector
from pipelines import upload_file_to_s3, send_sqs_message_from_json, zip_folder


load_dotenv()

class ProductCrawler:
    def __init__(self, proxy_key, name):
        self.proxy_key = proxy_key
        self.name = name
        self.crawl_timestamp = "%d%m%Y_%H%M%S"
        self.proxy_manager = RotatingProxy(proxy_key)
        self.current_proxy = None
        self.last_proxy_change = 0
        self.total_saved_count = 0
        self.stop_event = None

    def get_proxy_config(self):
        current_time = time.time()
        elapsed_time = current_time - self.last_proxy_change

        def try_get_valid_proxy():
            for attempt in range(3):
                proxy_info = self.proxy_manager.get_new_proxy()
                if proxy_info:
                    return proxy_info
                else:
                    print(f"‚ö†Ô∏è {self.name} - Proxy ch∆∞a s·∫µn s√†ng, th·ª≠ l·∫°i sau 60 gi√¢y...")
                    time.sleep(60)
            print(f"‚ùå {self.name} - Kh√¥ng th·ªÉ l·∫•y proxy h·ª£p l·ªá sau nhi·ªÅu l·∫ßn th·ª≠.")
            return None

        # ‚ö†Ô∏è N·∫øu ch∆∞a ƒë·ªß 60 gi√¢y th√¨ ƒë·ª£i cho ƒë·∫øn khi ƒë·ªß
        if self.current_proxy and elapsed_time < 60:
            wait_time = int(60 - elapsed_time)
            print(f"‚è≥ {self.name} - Ch∆∞a ƒë·ªß th·ªùi gian ƒë·ªïi proxy. ƒêang ch·ªù {wait_time}s...")
            time.sleep(wait_time)

        # ƒê·ªïi proxy (sau khi ƒë√£ ƒë·ªß 60s)
        new_proxy = try_get_valid_proxy()
        if new_proxy:
            self.current_proxy = new_proxy
            self.last_proxy_change = time.time()
        else:
            print(f"‚ùå {self.name} - Kh√¥ng ƒë·ªïi ƒë∆∞·ª£c proxy, gi·ªØ nguy√™n proxy c≈© v√† ti·∫øp t·ª•c.")

        proxy = self.current_proxy
        if not proxy:
            raise Exception(f"üõë {self.name}Kh√¥ng c√≥ proxy n√†o kh·∫£ d·ª•ng ƒë·ªÉ ti·∫øp t·ª•c.")

        proxy_url = f"http://{proxy['username']}:{proxy['password']}@{proxy['ip']}:{proxy['port']}"
        return {
            'proxy': {'http': proxy_url, 'https': proxy_url},
            'verify_ssl': False,
            'suppress_connection_errors': True,
            'connection_timeout': 60
        }
    
    def get_random_user_agent(self):
        """Tr·∫£ v·ªÅ User-Agent ng·∫´u nhi√™n t·ª´ danh s√°ch"""
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Safari/605.1.15"
        ]
        return random.choice(user_agents)

    def setup_driver(self, proxy=None):
        """Kh·ªüi t·∫°o tr√¨nh duy·ªát Chrome v·ªõi options ph√π h·ª£p v√† proxy s·ª≠ d·ª•ng selenium-wire"""
        chrome_options = Options()
        # C·∫•u h√¨nh c∆° b·∫£n
        # chrome_options.add_argument("--headless=new")  # S·ª≠ d·ª•ng headless m·ªõi, √≠t b·ªã ph√°t hi·ªán h∆°n
        chrome_options.add_argument("--disable-notifications")
        chrome_options.add_argument("--disable-popup-blocking")
        chrome_options.add_argument(f"--lang=vi-VN")
        
        # Gi·∫£m t√†i nguy√™n
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--window-size=1280,720")  # K√≠ch th∆∞·ªõc nh·ªè h∆°n ƒë·ªÉ ti·∫øt ki·ªám RAM
        chrome_options.add_argument("--disable-audio-output")
        chrome_options.add_argument("--disk-cache-size=33554432")  # Cache 32MB
        chrome_options.add_argument("--js-flags=--expose-gc,--max_old_space_size=100")  # Gi·ªõi h·∫°n b·ªô nh·ªõ JS
        
        # ·∫®n t·∫•t c·∫£ log console c·ªßa Chrome
        chrome_options.add_argument("--log-level=3")  # FATAL
        chrome_options.add_experimental_option('excludeSwitches', ["enable-automation", "enable-logging"])
        
        # T√πy ch·ªçn n√¢ng cao ƒë·ªÉ tr√°nh ph√°t hi·ªán bot
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_experimental_option("prefs", {
            "profile.default_content_setting_values.geolocation": 2,  # 2 = block, 0 = allow, 1 = ask
            "credentials_enable_service": False,
            "profile.password_manager_enabled": False,
            "profile.default_content_setting_values.notifications": 2,
            # "profile.managed_default_content_settings.images": 2 
        })
        
        chrome_options.add_argument(f"user-agent={self.get_random_user_agent()}")
        
        # C·∫•u h√¨nh seleniumwire options
        seleniumwire_options = {
            'connection_timeout': 60,  # Gi·∫£m th·ªùi gian timeout
            'verify_ssl': False,       # T·∫Øt x√°c minh SSL ƒë·ªÉ tƒÉng t·ªëc
            'suppress_connection_errors': True
        }
        
        # Th√™m c·∫•u h√¨nh proxy n·∫øu ƒë∆∞·ª£c cung c·∫•p
        if proxy:
            seleniumwire_options.update(proxy)


        # ‚ö†Ô∏è C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø c·ªßa b·∫°n ·ªü ƒë√¢y
        

        # chrome_options.binary_location = os.getenv('CHROME_BINARY_PATH')

        chromedriver_autoinstaller.install()

        driver = webdriver.Chrome(
            options=chrome_options,
            seleniumwire_options=seleniumwire_options
        )

        # driver = webdriver.Chrome(
        #     service=Service(os.getenv('CHROMEDRIVER_PATH')),
        #     options=chrome_options,
        #     seleniumwire_options=seleniumwire_options
        # )


        # T·∫°o driver v·ªõi selenium-wire
        # driver = webdriver.Chrome(options=chrome_options, seleniumwire_options=seleniumwire_options)
        
        # Thi·∫øt l·∫≠p page load timeout h·ª£p l√Ω
        driver.set_page_load_timeout(30)
        
        # C√†i ƒë·∫∑t Stealth JavaScript
        stealth_js = """
        // ·∫®n WebDriver
        Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
        
        // ·∫®n Chrome automation
        window.navigator.chrome = { runtime: {} };
        
        // Gi·∫£ l·∫≠p ƒëi·ªÉm chu·ªôt
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
            Promise.resolve({ state: Notification.permission }) :
            originalQuery(parameters)
        );
        
        // ·∫®n plugins v√† mime types
        Object.defineProperty(navigator, 'plugins', {
            get: () => [
                {
                    0: {type: "application/pdf"},
                    description: "Portable Document Format",
                    suffixes: "pdf",
                    type: "application/pdf",
                    name: "PDF Viewer"
                }
            ]
        });
        
        // Gi·∫£ l·∫≠p ng√¥n ng·ªØ th·ª±c t·∫ø
        Object.defineProperty(navigator, 'languages', {
            get: () => ['vi-VN', 'vi', 'en-US', 'en']
        });
        """
        driver.execute_script(stealth_js)
        
        return driver

    def generate_product_id(self, url):
        from hashlib import md5
        parsed = urllib.parse.urlparse(url)
        norm_url = parsed.netloc + parsed.path.rstrip('/')
        return md5((norm_url + str(len(parsed.path))).encode()).hexdigest()

    def generate_timestamped_folder(self, prefix, base_dir):
        time_str = self.crawl_timestamp
        folder_name = f"{prefix}_{time_str}_{os.getenv('MACHINE')}"
        return os.path.join(base_dir, folder_name)

    def update_storage_folders(self):
        self.image_folder = self.generate_timestamped_folder("image", "image")
        self.json_folder = self.generate_timestamped_folder("json", "json")
        os.makedirs(self.image_folder, exist_ok=True)
        os.makedirs(self.json_folder, exist_ok=True)

    def download_image(self, url, path, proxies, retries=3, delay=2):
        for attempt in range(1, retries + 1):
            try:
                r = requests.get(url, proxies=proxies, timeout=10)
                if r.status_code == 200 and len(r.content) > 100:
                    with open(path, 'wb') as f:
                        f.write(r.content)
                    return True
            except:
                pass
            time.sleep(delay)
        return False

    def clean_price(self, price_str):
        if not price_str:
            return None
        price_digits = re.sub(r'[^0-9]', '', price_str)
        try:
            return int(price_digits)
        except:
            return None

    def parse_rating(self, rating_str):
        try:
            return float(rating_str.split('/')[0].strip())
        except:
            return None
        
    def save_product(self, product):
        pid = self.generate_product_id(product['store_url'])
        product["id"] = pid
        ext = ".jpg"
        image_filename = f"{pid}{ext}"
        image_path = os.path.join(self.image_folder, image_filename)
        json_path = os.path.join(self.json_folder, f"{pid}.json")
        

        s3_image_key = f"images/{os.path.basename(self.image_folder)}/{image_filename}"
        s3_json_key = f"jsons/{os.path.basename(self.json_folder)}/{pid}.json"
        s3_url = f"https://e-commerce-data-lake.s3.{os.getenv('AWS_REGION')}.amazonaws.com/{s3_image_key}"

        image_exists = os.path.exists(image_path)
        json_exists = os.path.exists(json_path)

        if image_exists and json_exists:
            print(f"‚ö†Ô∏è {self.name} - S·∫£n ph·∫©m ƒë√£ t·ªìn t·∫°i ƒë·∫ßy ƒë·ªß: {pid}, b·ªè qua")
            return False

        # L∆∞u l·∫°i URL ·∫£nh g·ªëc ƒë·ªÉ t·∫£i, v√¨ s·∫Ω thay ƒë·ªïi image_url th√†nh S3 sau khi t·∫£i xong
        original_image_url = product["image_url"]

        if not image_exists:
            if not self.download_image(original_image_url, image_path, self.current_proxy_config.get('proxy')):
                print(f"‚ùå {self.name} - Kh√¥ng t·∫£i ƒë∆∞·ª£c ·∫£nh: {original_image_url}")
                return False
            else:
                print(f"üñº {self.name} - ƒê√£ t·∫£i ·∫£nh: {image_path}")

        # ‚úÖ Sau khi t·∫£i ·∫£nh th√†nh c√¥ng, c·∫≠p nh·∫≠t l·∫°i image_url th√†nh S3
        product["image_url"] = s3_url

        if not json_exists:
            try:
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(product, f, ensure_ascii=False, indent=2)
                print(f"‚úÖ {self.name} - ƒê√£ l∆∞u JSON: {json_path}")
            except Exception as e:
                print(f"‚ùå {self.name} - L·ªói khi l∆∞u JSON: {e}")
                return False

        # ‚úÖ Upload ·∫£nh v√† JSON l√™n S3
        try:
            upload_file_to_s3(image_path, s3_path=f"images/{os.path.basename(self.image_folder)}/", crawler_name=self.name)
            upload_file_to_s3(json_path, s3_path=f"jsons/{os.path.basename(self.json_folder)}/", crawler_name=self.name)
        
            # ‚úÖ Upload JSON l√™n SQS
            send_sqs_message_from_json(json_path)
        except Exception as e:
            print(f"‚ùå {self.name} - Kh√¥ng th·ªÉ upload l√™n S3: {e}")
            return False

        return True



    def crawl_keyword(self, keyword):
        self.crawl_timestamp = datetime.now().strftime("%d%m%Y_%H%M%S")
        print(f"[{self.name}] üëâ Crawl keyword: {keyword} ‚Üí t·∫°o folder timestamp {self.crawl_timestamp}")
        self.update_storage_folders()

        sw_options = self.get_proxy_config()
        self.current_proxy_config = sw_options
        driver = self.setup_driver(sw_options)

        self.last_saved_count = 0  # <-- l∆∞u s·ªë l∆∞·ª£ng s·∫£n ph·∫©m th√†nh c√¥ng cho m·ªói keyword

        try:
            url = f"https://www.google.com/search?q={urllib.parse.quote(keyword)}&tbm=shop&hl=vi&gl=vn"
            driver.get(url)

            page_source = driver.page_source
            if "recaptcha" in page_source or "captcha-form" in page_source:
                print(f"{self.name} - Ph√°t hi·ªán CAPTCHA. B·ªè qua keyword: {keyword}")
                return False  # Kh√¥ng ack ƒë·ªÉ requeue

            simulator = HumanBehaviorSimulator(driver)
            simulator.perform_random_action()

            handled = False
            total_saved = 0

            selector_handlers = [
                ("div.MtXiu", self.process_cards_mtXiu),
                ("div.LrTUQ", self.process_cards_LrTUQ)
            ]

            for selector, handler in selector_handlers:
                if self.stop_event.is_set():
                    print(f"{self.name}: Nh·∫≠n t√≠n hi·ªáu d·ª´ng, tho√°t kh·ªèi crawl_keyword.")
                    break

                try:
                    cards = WebDriverWait(driver, 30).until(
                        EC.presence_of_all_elements_located((By.CSS_SELECTOR, selector))
                    )
                    print(f"{self.name}: Found {len(cards)} products")
                    if cards:
                        print(f"{self.name} start crawling by {selector} card")
                        total_saved = handler(driver, cards, simulator)
                        handled = True
                        break
                except TimeoutException:
                    pass

            try:
                for folder in [self.image_folder, self.json_folder]:
                    zip_path = zip_folder(folder)
                    if zip_path:
                        if 'json' in folder:
                            s3_subpath = f"archives/json/{os.path.basename(zip_path)}"
                        elif 'image' in folder:
                            s3_subpath = f"archives/image/{os.path.basename(zip_path)}"
                        else:
                            s3_subpath = f"archives/{os.path.basename(zip_path)}"
                        upload_file_to_s3(zip_path, s3_path=s3_subpath, crawler_name=self.name)
            except Exception as e:
                print(f"‚ùå {self.name} - L·ªói khi zip/upload folder sau crawl: {e}")

            self.last_saved_count = total_saved  # <-- c·∫≠p nh·∫≠t s·ªë l∆∞·ª£ng s·∫£n ph·∫©m
            return handled and total_saved > 0
        
        except:
            return False
        finally:
            driver.quit()

    def process_cards_mtXiu(self, driver, cards, simulator):
        for i, card in enumerate(cards):
            if self.stop_event.is_set():
                print(f"{self.name}: Nh·∫≠n t√≠n hi·ªáu d·ª´ng, tho√°t gi·ªØa danh s√°ch s·∫£n ph·∫©m MtXiu.")
                break

            product = {}
            try:
                driver.execute_script("arguments[0].scrollIntoView({behavior: 'auto', block: 'center'});", card)
                ActionChains(driver).move_to_element(card).pause(2).click().perform()
                WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.mLFOe")))
                simulator.simulate_reading()

                product['timestamp'] = self.crawl_timestamp
                product['image_url'] = driver.find_element(By.CSS_SELECTOR, "div.Cl9jQc img.KfAt4d").get_attribute('src')
                product['name'] = driver.find_element(By.CSS_SELECTOR, "h2.u44bxd").text
                product['store_url'] = driver.find_element(By.XPATH, "//div[contains(@class, 'sCXXQd')]/a").get_attribute('href')

                price_element = card.find_element(By.CSS_SELECTOR, "span.lmQWe")
                product['price'] = price_element.text

                rating_raw = card.find_element(By.CSS_SELECTOR, "span.yi40Hd").text.strip()
                product['rating'] = rating_raw

                reviews_text = driver.find_element(By.CSS_SELECTOR, "span.QJUAn").text
                match = re.search(r'(\d+\.?\d*[KM]?)', reviews_text)
                product['reviews_count'] = match.group(1) if match else "0"

                if self.save_product(product):
                    self.total_saved_count += 1  # ‚úÖ c·∫≠p nh·∫≠t bi·∫øn to√†n c·ª•c

            except:
                pass

    def process_cards_LrTUQ(self, driver, cards, simulator):
        for i, card in enumerate(cards):
            if self.stop_event.is_set():
                print(f"{self.name}: Nh·∫≠n t√≠n hi·ªáu d·ª´ng, tho√°t gi·ªØa danh s√°ch s·∫£n ph·∫©m LrTUQ.")
                break
            product = {}
            try:
                driver.execute_script("arguments[0].scrollIntoView({behavior: 'auto', block: 'center'});", card)
                ActionChains(driver).move_to_element(card).pause(0.5).click().perform()
                WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.mLFOe")))
                simulator.simulate_reading()

                product['timestamp'] = self.crawl_timestamp
                product['image_url'] = driver.find_element(By.CSS_SELECTOR, "div.Cl9jQc img.KfAt4d").get_attribute('src')
                product['name'] = driver.find_element(By.CSS_SELECTOR, "div.bi9tFe").text
                product['store_url'] = driver.find_element(By.XPATH, "//div[contains(@class, 'sCXXQd')]/a").get_attribute('href')

                price_element = card.find_element(By.CSS_SELECTOR, "span.lmQWe")
                product['price'] = price_element.text

                rating_raw = card.find_element(By.CSS_SELECTOR, "span.yi40Hd").text.strip()
                product['rating'] = rating_raw

                reviews_text = driver.find_element(By.CSS_SELECTOR, "span.Bk5Fre").text
                match = re.search(r'(\d+\.?\d*[KM]?)', reviews_text)
                product['reviews_count'] = match.group(1) if match else "0"

                if self.save_product(product):
                    self.total_saved_count += 1  # ‚úÖ c·∫≠p nh·∫≠t bi·∫øn to√†n c·ª•c

                if i % 5 == 0:
                    simulator.perform_random_action()
            except:
                pass

    def process_cards_new_style(self, driver, cards, simulator):
        pass

    def run(self, stop_event):
        self.stop_event = stop_event

        try:
            def callback(keyword):
                if self.stop_event.is_set():
                    print(f"{self.name} d·ª´ng ti√™u th·ª• keyword v√¨ c√≥ y√™u c·∫ßu tho√°t.")
                    return False
                return self.crawl_keyword(keyword)

            connector = RabbitMQConnector()
            self.connector = connector
            connector.start_safe_consume("keywords", callback)
        except Exception as e:
            print(f"‚ùå {self.name} g·∫∑p l·ªói trong run(): {e}")